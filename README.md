<h1 align="center">üî•üî•üî• Awesome-LLM-Ensemble

"Harnessing Multiple Large Language Models: A Survey on LLM Ensemble"  (ArXiv 2025) </h2>

<p align="center">
    <a href="https://zhijunchen-ai.github.io/">Zhijun Chen</a>,
    <a href="https://scholar.google.com/citations?user=6v6JfdsAAAAJ&hl=zh-en">Jingzheng Li</a>,
    <a href="https://scholar.google.com/citations?user=mzXg1s8AAAAJ&hl=zh-en">Pengpeng Chen</a>,
    <a href="https://lizhuoran-nlp.github.io/">Zhuoran Li</a>,
    <a href="https://scholar.google.com/citations?user=buUlnJUAAAAJ&hl=zh-en">Kai Sun</a>,
    <a href="https://luoyk1999.github.io/">Yuankai Luo</a>,
    <a href="https://scholar.google.com/citations?hl=zh-CN&user=PnDqlPkAAAAJ&view_op=list_works&sortby=pubdate">Qianren Mao</a>,
    <a href="https://scholar.google.com/citations?user=H3pKglUAAAAJ&hl=zh-en">Ming Li</a>,    
    <a href="https://www.researchgate.net/scientific-contributions/Likang-Xiao-2257895822">Likang Xiao</a>,
    <a href="https://scholar.google.co.uk/citations?user=fnfg9S0AAAAJ&hl=en">Dingqi Yang</a>,
    <a href="https://www.banyikun.com/">Yikun Ban</a>,
    <a href="https://scholar.google.com/citations?user=HWOWCdcAAAAJ&hl=zh-CN">Hailong Sun</a>,
    <a href="https://scholar.google.com/citations?user=D0lL1r0AAAAJ&hl=zh-CN/">Philip S. Yu</a>
</p>
<p align="center">
    <img src="fig/logo.png" alt="Logo" width="=80%">
</p>









<p align="center">
  <a href='https://arxiv.org/abs/2502.18036'><img src='https://img.shields.io/badge/Arxiv-2502.18036-b31b1b.svg?logo=arXiv'></a>
  <a href="https://junchenzhi.github.io/LLM-Ensemble/">
    <img src="https://img.shields.io/badge/Website-Visit%20Now-purple" alt="Website">
  </a>
  <a href="https://mp.weixin.qq.com/s/yVWzHQmr_KyyOfY5k3ivOw">
    <img src="https://img.shields.io/badge/Blog-(Chinese)-orange" alt="Blog (Chinese)">
  </a>
</p>


<p align="center">
      <a href="https://github.com/junchenzhi/Awesome-LLM-Ensemble/commits/main">
        <img src="https://img.shields.io/badge/Maintained%3F-yes-green.svg" alt="Maintenance">
      </a>
      <a href="https://github.com/junchenzhi/Awesome-LLM-Ensemble/blob/main/LICENSE">
        <img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" alt="License">
      </a>
      <a href="https://awesome.re">
        <img src="https://awesome.re/badge.svg" alt="Awesome">
      </a>
  <a href="https://img.shields.io/badge/PRs-Welcome-red">
    <img src="https://img.shields.io/badge/PRs-Welcome-red" alt="PRs Welcome">
  </a>
  <a href=""><img src="https://img.shields.io/github/last-commit/junchenzhi/Awesome-LLM-Ensemble?color=lightgrey"></a>
</p>


<p align="center">
      <a href="https://github.com/junchenzhi/Awesome-LLM-Ensemble/stargazers">
        <img src="https://img.shields.io/github/stars/junchenzhi/Awesome-LLM-Ensemble?color=green" alt="GitHub stars">
      </a>
      <a href="https://github.com/junchenzhi/Awesome-LLM-Ensemble/network">
        <img src="https://img.shields.io/github/forks/junchenzhi/Awesome-LLM-Ensemble?color=blue&label=Forks" alt="GitHub forks">
      </a>
</p>



<h5 align="center">If you like our project, please give it a star ‚≠ê to show your supportÔºÅThank you:)

</h5>





# üì£ Notices
> üî•üî•üî• This is a collection of papers on  ***LLM Ensemble***.  
> <a href='https://arxiv.org/abs/2502.18036'>
>   <img src='https://img.shields.io/badge/Arxiv-2502.18036-b31b1b.svg?logo=arXiv'>
> </a>
> <a href="https://junchenzhi.github.io/LLM-Ensemble/">
>   <img src="https://img.shields.io/badge/Website-Visit%20Now-purple" alt="Website">
> </a>
> <a href="https://mp.weixin.qq.com/s/yVWzHQmr_KyyOfY5k3ivOw">
>   <img src="https://img.shields.io/badge/Blog-(Chinese)-orange" alt="Blog (Chinese)">
> </a>




> **[Always] [Add your paper in this repo]** ***Thank you to all the papers that have cited our survey! 
We will add all related citing papers to this GitHub repo, in a timely manner, to help increase the visibility of your contributions.***     
>


> **[Always] [Maintain]** ***We will make this list updated frequently (at least until 12/31/2025)!***     
> If you found any error or any missed/new paper, please don't hesitate to contact [us](zhijunchen@buaa.edu.cn) or Pull requests. 

---



&nbsp; 
&nbsp;  
&nbsp; 




- [Contents](#Awesome-LLM-Ensemble)
  - [1. LLM Ensemble and Taxonomy](#1-llm-ensemble-and-taxonomy)
    - [1.1 LLM Ensemble](#11-llm-ensemble)
    - [1.2 Taxonomy](#12-taxonomy)
  - [2. Papers](#2-papers)
    - [2.1 Ensemble Before Inference](#21-ensemble-before-inference)
      - [2.1.1 (a,1) Pre-Trained Router](#211-a1-pre-trained-router)
      - [2.1.2 (a,2) Non pre-trained router](#212-a2-non-pre-trained-router)
    - [2.2 Ensemble During Inference](#22-ensemble-during-inference)
      - [2.2.1 (b,1) Token-Level Ensemble](#221-b1-token-level-ensemble)
      - [2.2.2 (b,2) Span-Level Ensemble](#222-b2-span-level-ensemble)
      - [2.2.3 (b,3) Process-Level Ensemble](#223-b3-process-level-ensemble)
    - [2.3 Ensemble After Inference](#23-ensemble-after-inference)
      - [2.3.1 (c,1) Non Cascade](#231-c1-non-cascade)
      - [2.3.2 (c,2) Cascade](#232-c2-cascade)
    - [2.4 Others: Benchmarks, Applications and Related Surveys](#24-others-benchmarks-applications-and-related-surveys)
      - [2.4.1 Benchmarks](#241-benchmarks)
      - [2.4.2 Applications](#242-applications)
      - [2.4.3 Related Surveys](#243-related-surveys)
  - [3 Others: Some public implementations of the LLM Ensemble methods](#3-others-some-public-implementations-of-the-llm-ensemble-methods)
  - [4 Others: Some other related interesting papers](#4-others-some-other-related-interesting-papers)
  - [5 Summarization](#5-summarization)
  - [6 Citing This Paper](#6-citing-this-paper)





# 1. LLM Ensemble and Taxonomy

## 1.1 LLM Ensemble
**Paper Abstract**:

LLM Ensemble---which involves the comprehensive use of multiple large language models (LLMs), each aimed at handling user queries during the downstream inference, to benefit from their individual strengths---has gained substantial attention recently. 
The widespread availability of LLMs, coupled with their varying strengths and out-of-the-box usability, has profoundly advanced the field of LLM Ensemble. 
This paper presents the first systematic review of recent developments in LLM Ensemble. 
First, we introduce our taxonomy of LLM Ensemble and discuss several related research problems. 
Then, we provide a more in-depth classification of methods under the broad categories of ``ensemble-before-inference, ensemble-during-inference, ensemble-after-inference'', and review all relevant methods. Finally, we introduce related benchmarks and applications, summarize existing studies, and suggest several future research directions. 
A curated list of papers  on LLM Ensemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.

## 1.2 Taxonomy

<div align=center>
<img src="./fig/illustration.png" width="95%">

**Figure 1:  Illustration of LLM Ensemble Taxonomy.** (Note that for *(b) ensemble-during-inference* paradigm, there is also a *process-level ensemble* approach that we have not represented in the figure, mainly because that this approach is instantiated by a single method.)
</div>



<div align=center>
<img src="./fig/all_methods.png" width="95%">

**Figure 2: Taxonomy of All LLM Ensemble Methods. (Please note that this figure may not be fully updated to include all the papers listed below.)**
</div>





- ***(a) Ensemble before inference.***  
In essence, this approach employs a routing algorithm prior to LLM inference to allocate a specific query to the most suitable model, allowing the selected model that is specialized for the query and typically more cost-efficient inference to perform the task.
 Existing methods can be classified into two categories, depending on whether the router necessitates the use of pre-customized data for pre-training:   
  - ***(a1) Pre-training router;***
  - ***(a2) Non pre-training router.***




- ***(b) Ensemble during inference.***  
As the most granular form of ensemble among the three broad categories, this type of approach encompasses: 
  - ***(b1) Token-level ensemble*** methods, which integrate the token-level outputs of multiple models at the finest granularity of decoding;
  - ***(b2) Span-level ensemble*** methods, which conduct ensemble at the level of a sequence fragment (e.g., a span of four words); 
  - ***(b3) Process-level ensemble*** methods, which select the optimal reasoning process step-by-step within the reasoning chain for a given complex reasoning task. 
Note that for these ensemble-during-inference methods, the aggregated text segments will be concatenated with the previous text and fed again to models.



- ***(c) Ensemble after inference.***  
These methods can be classified into two categories:
  - ***(c1) Non cascade*** methods, which perform ensemble using multiple complete responses contributed from all LLM candidates;
  - ***(c2) Cascade*** methods, which consider both performance and inference costs, progressively reasoning through a chain of LLM candidates largely sorted by model size to find the most suitable inference response.




---



&nbsp; 
&nbsp;  
&nbsp; 



# 2. Papers

## 2.1 Ensemble Before Inference


<div align=center>
<img src="./fig/before.png" width="90%">

Figure 3:  Summary analysis of the key attributes of ensemble-before-inference methods.  (Please note that this table may not be fully updated to include all the papers listed below.)
</div>





### 2.1.1 (a,1) Pre-Trained Router





- **LLM Routing with Benchmark Datasets.** (2023) <a href="https://openreview.net/forum?id=k9EfAJhFZc"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: -, Code: -

- **RouteLLM: Learning to Route LLMs with Preference Data.** (2024) <a href="https://arxiv.org/abs/2406.18665"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: RouteLLM, <a href="https://github.com/lm-sys/RouteLLM"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing.** (2024) <a href="https://arxiv.org/abs/2404.14618"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Hybrid-LLM, <a href="https://github.com/m365-core/hybrid_llm_routing"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **LLM Bandit: Cost-Efficient LLM Generation via Preference-Conditioned Dynamic Routing.** (2025) <a href="https://arxiv.org/abs/2502.02743"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: -, Code: -

- **Harnessing the Power of Multiple Minds: Lessons Learned from LLM Routing.** (2024) <a href="https://arxiv.org/abs/2405.00467"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: -, <a href="https://github.com/kvadityasrivatsa/llm-routing"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **MetaLLM: A High-performant and Cost-efficient Dynamic Framework for Wrapping LLMs.** (2024) <a href="https://arxiv.org/abs/2407.10834"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: MetaLLM, <a href="https://github.com/mail-research/MetaLLM-wrapper/"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language Models.** (2024) <a href="https://arxiv.org/abs/2408.08545"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: SelectLLM, Code: -

- **Bench-CoE: a Framework for Collaboration of Experts from Benchmark.** (2024) <a href="https://arxiv.org/abs/2412.04167"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Bench-CoE, <a href="https://github.com/ZhangXJ199/Bench-CoE"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models.** (2023) <a href="https://arxiv.org/abs/2311.08692"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: ZOOTER, Code: -

- **TensorOpera Router: A Multi-Model Router for Efficient LLM Inference.** (2024) <a href="https://arxiv.org/abs/2408.12320"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: TO-Router, Code: -

- **Query Routing for Homogeneous Tools: An Instantiation in the RAG Scenario.** (2024) <a href="https://arxiv.org/abs/2406.12429"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: HomoRouter, Code: -

- **Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling.** (2023) <a href="https://arxiv.org/abs/2308.06077"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: FORC, <a href="https://github.com/epfl-dlab/forc"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **Routoo: Learning to Route to Large Language Models Effectively.** (2024) <a href="https://arxiv.org/abs/2401.13979"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Routoo, Code: -

- **(Newly added paper, March 2025:) RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling Large Language Models.** (2024) <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/7a641b8ec86162fc875fb9f6456a542f-Abstract-Conference.html"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: RouterDC, <a href="https://github.com/shuhao02/RouterDC"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **(Newly added paper, April 2025:) An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing.** (2024) <a href="https://arxiv.org/abs/2403.16854"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: ETR, <a href="https://github.com/zjunet/ETR"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **(Newly added paper, May 2025:) Rethinking Predictive Modeling for LLM Routing: When Simple kNN Beats Complex Learned Routers.** (2025) <a href="https://arxiv.org/abs/2505.12601"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: -, Code: -

- **(Newly added paper, May 2025:) InferenceDynamics: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling.** (2025) <a href="https://arxiv.org/abs/2505.16303"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: InferenceDynamics, Code: -

- **(Newly added paper, May 2025:) CO-OPTIMIZING RECOMMENDATION AND EVALUATION FOR LLM SELECTION.** (2025) <a href="https://openreview.net/pdf?id=gWi4ZcPQRl"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: RELM, Code: -

- **(Newly added paper, May 2025:) Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection.** (2025) <a href="https://arxiv.org/abs/2505.19435"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: RTR, <a href="https://github.com/goodmanpzh/Route-To-Reason"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **(Newly added paper, June 2025:) RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing.** (2025) <a href="https://www.arxiv.org/abs/2506.03880"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: RadialRouter, Code: -


- **(Newly added paper, June 2025:) TAGROUTER: Learning Route to LLMs through Tags for Open-Domain Text Generation Tasks.** (2025) <a href="https://arxiv.org/abs/2506.12473"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: TagRouter, Code: -


- **(Newly added paper, October 2025:) LLMRank: Understanding LLM Strengths for Model Routing.** (2025) <a href="https://arxiv.org/abs/2510.01234"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: LLMRank, Code: -



- **(Newly added paper, October 2025:) GraphRouter: A Graph-based Router for LLM Selections.** (2025) <a href="https://arxiv.org/abs/2410.03834"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: GraphRouter, Code: <a href="https://github.com/ulab-uiuc/GraphRouter"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>



- **(Newly added paper, October 2025:) Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning.** (2025) <a href="https://arxiv.org/abs/2506.09033"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Router-R1, Code: <a href="https://github.com/ulab-uiuc/Router-R1"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>



- **(Newly added paper, October 2025:) DISROUTER: DISTRIBUTED SELF-ROUTING FOR LLM SELECTIONS.** (2025) <a href="https://arxiv.org/abs/2510.19208"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: DiSRouter, Code: -



### 2.1.2 (a,2) Non pre-trained router


- **PickLLM: Context-Aware RL-Assisted Large Language Model Routing.** (2024) <a href="https://arxiv.org/abs/2412.12170"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: PickLLM, Code: -

- **Eagle: Efficient Training-Free Router for Multi-LLM Inference.** (2024) <a href="https://arxiv.org/abs/2409.15518"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Eagle, Code: -

- **Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM.** (2024) <a href="https://arxiv.org/abs/2401.02994"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Blending, Code: -


- **(Newly added paper, June 2025:) The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants.** (2025) <a href="https://arxiv.org/abs/2505.19797"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Avengers, <a href="https://github.com/ZhangYiqun018/Avengers"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>





## 2.2 Ensemble During Inference

<div align=center>
<img src="./fig/during.png" width="90%">

Figure 4:  Summary analysis of the key attributes of ensemble-during-inference methods.  (Please note that this table may not be fully updated to include all the papers listed below.)
</div>



### 2.2.1 (b,1) Token-Level Ensemble
- **Breaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling.** (2024) <a href="https://arxiv.org/abs/2406.12585"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: GaC, <a href="https://github.com/yaoching0/GaC"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **Ensemble Learning for Heterogeneous Large Language Models with Deep Parallel Collaboration.** (2024) <a href="https://arxiv.org/abs/2404.12715"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: DeePEn, <a href="https://github.com/OrangeInSouth/DeePEn"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **Bridging the Gap between Different Vocabularies for LLM Ensemble.** (2024) <a href="https://arxiv.org/abs/2404.09492"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: EVA, <a href="https://github.com/xydaytoy/EVA"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **Determine-Then-Ensemble: Necessity of Top-k Union for Large Language Model Ensembling.** (2024) <a href="https://arxiv.org/abs/2410.03777"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: UniTe, Code: -

- **Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization.** (2024) <a href="https://arxiv.org/abs/2404.11531"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: PackLLM, <a href="https://github.com/cmavro/PackLLM"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **Purifying large language models by ensembling a small language model.** (2024) <a href="https://arxiv.org/abs/2402.14845"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: -, Code: -

- **CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing.** (2025) <a href="https://arxiv.org/abs/2502.01976"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: CITER, <a href="https://github.com/aiming-lab/CITER"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **(Newly added paper, April 2025:) Speculative Ensemble: Fast Large Language Model Ensemble via Speculation.** (2025) <a href="https://arxiv.org/abs/2502.01662"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Speculative Ensemble, <a href="https://github.com/Kamichanw/Speculative-Ensemble/"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **(Newly added paper, September 2025:) Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning.** (2025) <a href="https://arxiv.org/abs/2505.16270"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Transformer Copilot, <a href="https://github.com/jiaruzouu/TransformerCopilot"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>


- **(Newly added paper, September 2025:) Token-level Ensembling of Models with Different Vocabularies.** (2025) <a href="https://arxiv.org/abs/2502.21265"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: ABE, <a href="https://github.com/mjpost/abe"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>


### 2.2.2 (b,2) Span-Level Ensemble



- **Cool-Fusion: Fuse Large Language Models without Training.** (2024) <a href="https://arxiv.org/abs/2407.19807"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Cool-Fusion, Code: -

- **Hit the Sweet Spot! Span-Level Ensemble for Large Language Models.** (2024) <a href="https://arxiv.org/abs/2409.18583"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: SweetSpan, Code: -

- **SpecFuse: Ensembling Large Language Models via Next-Segment Prediction.** (2024) <a href="https://arxiv.org/abs/2412.07380"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: SpecFuse, Code: -

- **(Newly added paper, June 2025:) RLAE: Reinforcement Learning-Assisted Ensemble for LLMs.** (2025) <a href="https://arxiv.org/abs/2506.00439"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: RLAE, Code: -




### 2.2.3 (b,3) Process-Level Ensemble




- **Ensembling Large Language Models with Process Reward-Guided Tree Search for Better Complex Reasoning.** (2024) <a href="https://arxiv.org/abs/2412.15797"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: LE-MCTS, Code: -



&nbsp; 


## 2.3 Ensemble After Inference


<div align=center>
<img src="./fig/after.png" width="90%">

Figure 5:  Summary analysis of the key attributes of ensemble-after-inference methods.  (Please note that this table may not be fully updated to include all the papers listed below.)
</div>


### 2.3.1 (c,1) Non Cascade


- **More Agents Is All You Need.** (2024) <a href="https://arxiv.org/abs/2402.05120"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Agent-Forest, <a href="https://github.com/MoreAgentsIsAllYouNeed/AgentForest"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **Smoothie: Label Free Language Model Routing.** (2024) <a href="https://arxiv.org/abs/2412.04692"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Smoothie, <a href="https://github.com/HazyResearch/smoothie"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **Getting MoRE out of Mixture of Language Model Reasoning Experts.** (2023) <a href="https://arxiv.org/abs/2305.14628"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: MoRE, <a href="https://github.com/NoviScl/MoRE"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion.** (2023) <a href="https://arxiv.org/abs/2306.02561"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: LLM-Blender, <a href="https://github.com/yuchenlin/LLM-Blender"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity.** (2024) <a href="https://arxiv.org/abs/2410.03953"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: LLM-TOPLA, <a href="https://github.com/git-disl/llm-topla"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **URG: A Unified Ranking and Generation Method for Ensembling Language Models.** (2024) <a href="https://aclanthology.org/2024.findings-acl.261/"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: URG, Code: -

- **(Newly added paper, April 2025:) DFPE: A Diverse Fingerprint Ensemble for Enhancing LLM Performance.** (2025) <a href="https://arxiv.org/abs/2501.17479"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: DFPE, <a href="https://github.com/nivgold/DFPE"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **(Newly added paper, April 2025:) Two Heads are Better than One: Zero-shot Cognitive Reasoning via Multi-LLM Knowledge Fusion.** (2024) <a href="https://dl.acm.org/doi/abs/10.1145/3627673.3679744"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: MLKF, <a href="https://github.com/trueBatty/MLKF"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **(Newly added paper, April 2025:) Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for Heterogeneous Reasoning.** (2025) <a href="https://arxiv.org/abs/2503.05641"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Symbolic-MoE, <a href="https://github.com/dinobby/Symbolic-MoE/"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **(Newly added paper, April 2025:) BALANCING ACT: DIVERSITY AND CONSISTENCY IN LARGE LANGUAGE MODEL ENSEMBLES.** (2025) <a href="https://openreview.net/pdf?id=Dl6nkKKvlX"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: DMoA, Code: -


- **(Newly added paper, June 2025:) EL4NER: Ensemble Learning for Named Entity Recognition via Multiple Small-Parameter Large Language Models.** (2025) <a href="https://arxiv.org/abs/2505.23038"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: EL4NER, Code: -


- **(Newly added paper, September 2025:) LENS: Learning Ensemble Confidence from Neural States for Multi-LLM Answer Integration.** (2025) <a href="https://arxiv.org/abs/2507.23167"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: EL4NER, Code: -




- **(Newly added paper, September 2025:) CARGO: A Framework for Confidence-Aware Routing of Large Language Models.** (2025) <a href="https://arxiv.org/abs/2509.14899"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name:  CARGO, Code: -

- **(Newly added paper, September 2025:) LLM-Forest: Ensemble Learning of LLMs with Graph-Augmented Prompts for Data Imputation.** (2025) <a href="https://arxiv.org/abs/2410.21520"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name:  LLM-Forest, <a href="https://github.com/Xinrui17/LLM-Forest"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>



- **(Newly added paper, October 2025:) Explainable Fault Localization for Programming Assignments via LLM-Guided Annotation.** (2025) <a href="https://arxiv.org/pdf/2509.25676v1"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name:  FLAME, <a href="https://github.com/FLAME-FL/FLAME"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>




- **(Newly added paper, October 2025:) Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information.** (2025) <a href="https://www.arxiv.org/abs/2510.01499"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: OW/ISP, Code: -






### 2.3.2 (c,2) Cascade



- **EcoAssistant: Using LLM Assistant More Affordably and Accurately.** (2023) <a href="https://arxiv.org/abs/2310.03046"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: EcoAssistant, <a href="https://github.com/JieyuZ2/EcoAssistant"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning.** (2023) <a href="https://arxiv.org/abs/2310.03094"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: -, <a href="https://github.com/MurongYue/LLM_MoT_cascade"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **Model Cascading: Towards Jointly Improving Efficiency and Accuracy of NLP Systems.** (2022) <a href="https://arxiv.org/abs/2210.05528"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Model Cascading, Code: -

- **Cache & Distil: Optimising API Calls to Large Language Models.** (2023) <a href="https://arxiv.org/abs/2310.13561"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: neural caching, <a href="https://github.com/guillemram97/neural-caching"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **A Unified Approach to Routing and Cascading for LLMs.** (2024) <a href="https://arxiv.org/abs/2410.10347"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Cascade Routing, <a href="https://github.com/eth-sri/cascade-routing"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **When Does Confidence-Based Cascade Deferral Suffice?** (2023) <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/1f09e1ee5035a4c3fe38a5681cae5815-Abstract-Conference.html"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: -, Code: -

- **FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance.** (2023) <a href="https://arxiv.org/abs/2305.05176"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: FrugalGPT, Code: -

- **Language Model Cascades: Token-level uncertainty and beyond.** (2024) <a href="https://arxiv.org/abs/2404.10136"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: FrugalGPT, Code: -

- **AutoMix: Automatically Mixing Language Models.** (2023) <a href="https://arxiv.org/abs/2310.12963"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: AutoMix, <a href="https://github.com/automix-llm/automix"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **Dynamic Ensemble Reasoning for LLM Experts.** (2024) <a href="https://arxiv.org/abs/2412.07448"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: DER, Code: -

- **(Newly added paper, April 2025:) EMAFusionTM: A SELF-OPTIMIZING SYSTEM FOR SEAMLESS LLM SELECTION AND INTEGRATION.** (2025) <a href="https://arxiv.org/abs/2504.10681"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: EMAFusionTM, Code: -

- **(Newly added paper, June 2025:) Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling Efficiently Scales Test-Time Compute.** (2025) <a href="https://arxiv.org/abs/2504.00762"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: ModelSwitch, <a href="https://github.com/JianhaoChen-nju/ModelSwitch"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>



- **(Newly added paper, September 2025:) Semantic Agreement Enables Efficient Open-Ended LLM Cascades.** (2025) <a href="https://www.arxiv.org/abs/2509.21837"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: -, Code: -



&nbsp; 

## 2.4 Others: Benchmarks, Applications and Related Surveys

### 2.4.1 Benchmarks




- **LLM-BLENDER: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion.** (2023) <a href="https://arxiv.org/abs/2306.02561"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: MixInstruct, Evaluation Goal: Performance, <a href="https://yuchenlin.xyz/LLM-Blender/"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **RouterBench: A Benchmark for Multi-LLM Routing System.** (2024) <a href="https://arxiv.org/abs/2411.04424"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: RouterBench, Evaluation Goal: Performance and cost, <a href="https://github.com/withmartian/routerbench"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **(Newly added paper, April 2025:) RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs.** (2025) <a href="https://arxiv.org/abs/2503.10657"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Speculative Ensemble, <a href="https://github.com/MilkThink-Lab/RouterEval"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>


### 2.4.2 Applications


Beyond the methods presented before, the concept of LLM Ensemble has found applications in a variety of more specialized tasks and domains.
Here we give some examples:


- **Ensemble-Instruct: Generating Instruction-Tuning Data with a Heterogeneous Mixture of LMs.** (2023) <a href="https://arxiv.org/abs/2310.13961"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Ensemble-Instruct, Task: Instruction-Tuning Data Generation, <a href="https://github.com/IBM/ensemble-instruct"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **Bayesian Calibration of Win Rate Estimation with LLM Evaluators.** (2024) <a href="https://arxiv.org/abs/2411.04424"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: BWRS, Bayesian Dawid-Skene, Task: Win Rate Estimation, <a href="https://github.com/yale-nlp/bay-calibration-llm-evaluators"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **PromptMind Team at MEDIQA-CORR 2024: Improving Clinical Text Correction with Error Categorization and LLM Ensembles.** (2024) <a href="https://arxiv.org/abs/2405.08373"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: -, Task: SQL generation, Code: -

- **LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction.** (2024) <a href="https://arxiv.org/abs/2403.00863"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: -, Task: Product Attribute Value Extraction, Code: -

- **(Newly added paper, April 2025:) FuseGen: PLM Fusion for Data-generation based Zero-shot Learning.** (2024) <a href="https://arxiv.org/abs/2406.12527"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: FuseGen, Task: Data-generation, <a href="https://github.com/LindaLydia/FuseGen"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **(Newly added paper, April 2025:) On Preserving the Knowledge of Long Clinical Texts.** (2023) <a href="https://arxiv.org/abs/2311.01571"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: -, Task: Prediction tasks on long clinical notes, Code: -

- **(Newly added paper, April 2025:) Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and Self-Improving OCR.** (2025) <a href="https://arxiv.org/abs/2504.11101"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Consensus Entropy (CE), Task: Optical Character Recognition, Code: -


- **(Newly added paper, June 2025:) Beyond Monoliths: Expert Orchestration for More Capable, Democratic, and Safe Large Language Models.** (2025) <a href="https://arxiv.org/abs/2506.00051"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Expert Orchestration, Task: -, Code: -



- **(Newly added paper, October 2025:) Explainable Fault Localization for Programming Assignments via LLM-Guided Annotation.** (2025) <a href="https://arxiv.org/pdf/2509.25676v1"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name:  FLAME, <a href="https://github.com/FLAME-FL/FLAME"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>





### 2.4.3 Related Surveys



- **(Newly added paper, May 2025:) Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities.** (2024) <a href="https://arxiv.org/abs/2408.07666"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GitHub: <a href="https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **(Newly added paper, May 2025:) Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models.** (2024) <a href="https://arxiv.org/abs/2407.06089"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GitHub: -

- **(Newly added paper, May 2025:) A Survey on Collaborative Mechanisms Between Large and Small Language Models.** (2025) <a href="https://arxiv.org/abs/2505.07460"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GitHub: -

- **(Newly added paper, May 2025:) A comprehensive review on ensemble deep learning: Opportunities and challenges.** (2023) <a href="https://www.sciencedirect.com/science/article/pii/S1319157823000228"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GitHub: -

- **(Newly added paper, May 2025:) Doing More with Less ‚Äì Implementing Routing Strategies in Large Language Model-Based Systems: An Extended Survey.** (2025) <a href="https://arxiv.org/abs/2502.00409"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GitHub: -

- **(Newly added paper, May 2025:) A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning.** (2024) <a href="https://arxiv.org/abs/2408.07057"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GitHub: -

- **(Newly added paper, May 2025:) Deep Model Fusion: A Survey.** (2023) <a href="https://arxiv.org/abs/2309.15698"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GitHub: -

- **(Newly added paper, June 2025:) Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques.** (2025) <a href="https://www.arxiv.org/abs/2506.06579"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GitHub: -

- **(Newly added paper, July 2025:) Toward Edge General Intelligence with Multiple-Large Language Model (Multi-LLM): Architecture, Trust, and Orchestration.** (2025) <a href="https://arxiv.org/abs/2507.00672"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GitHub: -
---

&nbsp; 
&nbsp;  
&nbsp; 

## 3 Others: Some public implementations of the LLM Ensemble methods 


- **(Newly added, May 2025:) Ensemble-Hub.** (2025)   
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GitHub: <a href="https://github.com/Fzkuji/Ensemble-Hub"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>


---

&nbsp; 
&nbsp;  
&nbsp; 


## 4 Others: Some other related interesting papers 


Here we briefly list some related papers, which are either discovered by us or suggested by the authors to this repository. 
They mainly focus on **LLM Collaboration**.

- **(Newly added paper, June 2025:)  If Multi-Agent Debate is the Answer, What is the Question?** (2025) <a href="https://arxiv.org/abs/2502.08788"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: Heter-MAD,  Code: -

- **(Newly added paper, June 2025:)  Nature-Inspired Population-Based Evolution of Large Language Models** (2025) <a href="https://arxiv.org/abs/2503.01155"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: GENOME,   <a href="https://github.com/ZhangYiqun018/GENOME"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>




- **(Newly added paper, September 2025:)  LLM-Forest: Ensemble Learning of LLMs with Graph-Augmented Prompts for Data Imputation** (2024) <a href="https://arxiv.org/abs/2410.21520"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: LLM-Forest,   <a href="https://github.com/Xinrui17/LLM-Forest"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

  
- **(Newly added paper, September 2025:)  Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM** (2025) <a href="https://arxiv.org/abs/2508.07260"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: SLC,   <a href="https://github.com/Hhankyangg/SLC"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>


- **(Newly added paper, September 2025:)  Best-of-‚àû ‚Äì Asymptotic Performance of Test-Time Compute** (2025) <a href="https://arxiv.org/abs/2509.21091"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: SLC,   <a href="https://github.com/jkomiyama/BoInf-code-publish"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>

- **(Newly added paper, September 2025:)  MIXTURE OF THOUGHTS: LEARNING TO AGGREGATE WHAT EXPERTS THINK, NOT JUST WHAT THEY SAY** (2025) <a href="https://arxiv.org/abs/2509.21164"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: SLC,   <a href="https://github.com/jacobfa/mot"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>


- **(Newly added paper, October 2025:) Towards Scalable Oversight with Collaborative Multi-Agent Debate in Error Detection** (2025) <a href="https://arxiv.org/abs/2510.20963v1"><img src="https://img.shields.io/badge/Paper-red?logo=arxiv&logoColor=white"></a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Name: ColMAD,   <a href="https://github.com/jacobfa/mot"><img src="https://img.shields.io/badge/Code-Official-blue?logo=github"></a>


---

&nbsp; 
&nbsp;  
&nbsp; 

***************





## 5 Summarization 

<div align=center><img src="./fig/summary.png" width="90%">

Figure 6:  Summary analysis of the key attributes of LLM Ensemble approaches.
</div> 

---

&nbsp; 
&nbsp;  
&nbsp; 

## 6 Citing This Paper


```
@article{chen2025harnessing,
  title={Harnessing Multiple Large Language Models: A Survey on LLM Ensemble},
  author={Chen, Zhijun and Li, Jingzheng and Chen, Pengpeng and Li, Zhuoran and Sun, Kai and Luo, Yuankai and Mao, Qianren and Li, Ming and Xiao, Likang and Yang, Dingqi and Ban, Yikun and Sun, Hailong and Yu, Philip S},
  journal={arXiv preprint arXiv:2502.18036},
  year={2025}
}
```
